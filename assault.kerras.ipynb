{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Keras\n",
    "\n",
    "So excited for this module. This means progress. Imagine, 21 days ago I was just starting out into Deep Learning. Now I have a working knowledge in Neural Network basics and I am already able to create one with numpy. Onwards.\n",
    "\n",
    "Brief research on [Keras](https://keras.io/), its meant to be run on top of tensorlow to enable faster experimentation on Deep Neural Networks. This in line with the guiding principle of Keras: User friendliness - Designed for human beings not machines, user experience is upfront. Modularity - components of a neural network are modular in Keras that allows for fully-configurable modules that can be plugged together. Easy Extensibility - as an extension of Modularity (pun intended) new modules are possible and easy to create allowing possibilities on total control and advanced research. Work with Python - Keras has no separate models configuration files, models use Python code and therefore are compact and easier to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving on to Keras Basics\n",
    "\n",
    "The core data structure for Keras is a __model__, a way to organize layers. The simplest model type is _sequential_, which is a linear stack of layers. Complex architectures are also available through the [Keras functional API](https://keras.io/getting-started/functional-api-guide) that allows arbitrary graphs of layers, not just linear.\n",
    "\n",
    "For this exercise we go over the different Keras calls by creating an __AND gate__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Note that Keras is case sensitive. Or was it always case sensitive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all dependencies. For this one we will still use __numpy__ for the inputs since we need the array. We import __Sequential__ from keras.models as this would be our basic model. Then we import __Dense__ and __Activation__ from keras.layers.core. __Dense__ is used for _densely-connected NN layer_. From the documentation it implements h=activation(dot(input,weights_or_kernel)+bias) so its the basic y = sigmoid(wx+b), the simple perceptron. __Activation__ is used to invoke the _activation function_ to be used, keras already includes the basic activation functions like _sigmoid, tanh, ELU, SELU, RELU, hard sigmoid_ and [more](https://keras.io/activations/). More information can be found here in [Core Layers Documentation](https://keras.io/layers/core/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]).astype('float32')\n",
    "y = np.array([[0],[1],[1],[0]]).astype('float32')\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we provide our __features and targets__. For this example, we are going to do a simple __AND Gate__ and we have 4 sets of pairs since this is a 2-input AND Gate. For the output, obviously its binary even though its called float. We also included __encoding__ in the form of __to_categorical__ where we convert our class vector (integers) to binary class matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just define our __model__ as sequential which is a linear stack of layers. A good read for this is the [Guide](https://keras.io/getting-started/sequential-model-guide/) provided by keras for __sequential models__. In our case we could have named it __and__ instead of model but that is just a preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(32,input_dim=X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we add the first layer, where we define a __Dense__ network with __batch size__ equal to __32__ and __input_dim__ as the input shape which for this case is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the __activation function__ for our model which in this case is a __softmax__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define our next layer which in this case is the __output__. Here, if I understood my arguments correctly, 2 is the batch size for the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we also add an activation function for the output. Since we want the output to range between 0 to 1 we will use __sigmoid__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __compile__ method is needed to define the learning process. For the model to compile, you need three arguments: [__optimizer__](https://keras.io/optimizers/), [__loss function__](https://keras.io/losses/) and a _LIST_ of __metrics__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                96        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 162\n",
      "Trainable params: 162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cannot find any item regarding summary method in Keras. But based on the output, it just shows you the table of the model you have built as well as its interconnection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X, y, epochs = 100, verbose=0)  # Probably overkill of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __fit__ method is used to __train__ the model. Think of it as the same as train function that was used when we were in Numpy. We have our __Features__ and our __Targets__ then we can also include batch sizes, validation_split and more. We also define how many passes we want via __epochs__ which in this case means 1000 passes. Then __verbose__ is a setting where we can view the results as they go or just wait for the training to run in the background. Basically, its similar to the meter we used in _Sentiment Analysis_ where we are able to get _accuracy_ and _epoch count_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "4/4 [==============================] - 0s 500us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here we __evaluate__ our model for __loss value__ and __metrics value__. Since we have metrics for __accuracy__ in our compile, we get __accuracy__ as our output for the evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy = 0.75\n",
      "\n",
      "Predictions =\n",
      "[[0.48811308 0.47764763]\n",
      " [0.48274556 0.48457536]\n",
      " [0.4801547  0.48669934]\n",
      " [0.4821377  0.4898439 ]]\n"
     ]
    }
   ],
   "source": [
    "print('\\nAccuracy =', score[-1])\n",
    "print('\\nPredictions =')\n",
    "print(model.predict_proba(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just print out the evaluation we got earlier. We also used __predict_proba__ method which is not found in the Keras documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow]",
   "language": "python",
   "name": "conda-env-Tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
